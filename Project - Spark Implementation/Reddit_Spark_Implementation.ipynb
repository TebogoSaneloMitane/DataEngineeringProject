{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985385da-5b5a-4213-94e3-744bc1af4472",
   "metadata": {},
   "source": [
    "# Data Engineering I: Comparative Analysis of MapReduce and Spark on Reddit Data\n",
    "### *Spark implementation and Evaluation: Tove Gunnarsson and Tebogo Sanelo Mitane*\n",
    "\n",
    "Test cases:\n",
    "| Dataset sizes  | 1 node | 2 nodes | 4 nodes |\n",
    "| -----------    | ------ | ------- | ------- |\n",
    "| 50k    |      |      |      |      |\n",
    "| 100k   |      |      |      |      |\n",
    "| 200k   |      |      |      |      |\n",
    "| 400k   |      |      |      |      |\n",
    "| 800k ?   |      |      |      |      |\n",
    "\n",
    "Evaluation metrics: Execution time, CPU and Memory usage.\n",
    "\n",
    "Measurements are written to a ... file and imported to a separate notebook where plots and other stats are generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfa4c35-e6a5-4d0d-9f53-ebeaff96f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/15 07:12:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.156:7077\") \\\n",
    "        .appName(\"Group 35\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 1)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.executor.instances\", 4)\\\n",
    "        .config(\"spark.ui.port\", \"8082\")\\\n",
    "        .config(\"spark.cores.max\", 1)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1952b92-f8ff-4260-b401-a5b1ba11cb41",
   "metadata": {},
   "source": [
    "# Preprocessing & Processing functions\n",
    "The functions to execute the preprocessing and counting job, as well as perform the measurements have been optimized to run in parallel, by choosing functions like\n",
    " * F.expr() instead of .withColumn()\n",
    " * agg() instead of groupBy().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c5487f-e2b4-44ef-b258-71541cebaf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import time\n",
    "import psutil\n",
    "from pyspark.sql.functions import lower, regexp_replace, col, when, count, sum as spark_sum\n",
    "\n",
    "# Define country list\n",
    "countries = [\"China\", \"India\", \"United States\", \"Indonesia\", \"Pakistan\", \n",
    "             \"Brazil\", \"Nigeria\", \"Bangladesh\", \"Russia\", \"Mexico\"]\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Optimized preprocessing: drops nulls, replaces missing values, lowers text, and removes punctuation.\"\"\"\n",
    "    df = df.select(\"subreddit\", \"body\").na.fill({\"subreddit\": \"unknown\", \"body\": \"\"})\n",
    "    df = df.withColumn(\"body\", F.lower(F.regexp_replace(F.col(\"body\"), \"[^a-zA-Z0-9\\s]\", \"\")))\n",
    "    return df\n",
    "\n",
    "def mentions_counter(df):\n",
    "    \"\"\"Optimized category count using a single operation for all columns.\"\"\"\n",
    "    \n",
    "    exprs = [F.when(F.col(\"body\").contains(item.lower()), 1).otherwise(0).alias(item) for item in countries]\n",
    "    df = df.select(\"subreddit\", *exprs)\n",
    "\n",
    "    # Count distinct subreddits where each category is mentioned\n",
    "    subreddit_mentions = df.groupBy(\"subreddit\").agg(*[F.max(item).alias(item) for item in countries])\n",
    "    \n",
    "    # Sum up the unique subreddit counts across all subreddits\n",
    "    total_mentions = subreddit_mentions.agg(*[F.sum(item).alias(item) for item in countries])\n",
    "    \n",
    "    return subreddit_mentions, total_mentions\n",
    "\n",
    "\n",
    "def measure_performance(df, dataset_name, metrics_dict):\n",
    "    \"\"\"Measure execution time and system utilization.\"\"\"\n",
    "    cpu_before = psutil.cpu_percent(interval=None)\n",
    "    memory_before = psutil.virtual_memory().used / (1024 ** 3)  # Convert bytes to GB\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    subreddit_mentions, total_mentions = mentions_counter(df)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    cpu_after = psutil.cpu_percent(interval=None)\n",
    "    memory_after = psutil.virtual_memory().used / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "    memory_peak = psutil.Process().memory_info().rss / (1024 ** 3)  # Resident Set Size (RSS) in GB\n",
    "\n",
    "    cpu_usage = cpu_after - cpu_before\n",
    "    memory_usage = memory_after - memory_before\n",
    "\n",
    "    metrics_dict[dataset_name] = {\n",
    "        \"Execution Time (s)\": execution_time,\n",
    "        \"CPU Usage\": cpu_usage,\n",
    "        \"Memory Usage\": memory_usage,\n",
    "        \"Memory Peak\": memory_peak,\n",
    "    }\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c501a9c-928d-4e7c-b923-0766529f3d20",
   "metadata": {},
   "source": [
    "Reading the datasets and creating the 400k and 800k datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b92b048-ce30-4859-9a48-c591d328493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: reddit_50k.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: reddit_100k.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: reddit_200k.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 4) Performance evaluation loop\n",
    "import csv\n",
    "metrics_result = []  # Dictionary to store performance results\n",
    "\n",
    "dataset_paths = [\n",
    "    \"hdfs://192.168.2.156:9000/data/reddit/reddit_50k.json\",\n",
    "    \"hdfs://192.168.2.156:9000/data/reddit/reddit_100k.json\",\n",
    "    \"hdfs://192.168.2.156:9000/data/reddit/reddit_200k.json\"\n",
    "]\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    dataset_name = dataset_path.split(\"/\")[-1]  # Extract dataset name\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "\n",
    "    # Load dataset\n",
    "    reddit_df = spark_session.read.json(dataset_path)\n",
    "\n",
    "    # Preprocess data\n",
    "    reddit_df = preprocess_data(reddit_df)\n",
    "\n",
    "    # Measure performance & store results\n",
    "    performance_metrics = measure_performance(reddit_df, dataset_name, {})\n",
    "\n",
    "    metrics_result.append({\n",
    "        \"Dataset\" : dataset_name, \n",
    "        \"No. nodes\": 4, # <-------------- CHANGE FOR DIFFERENT NO. NODES\n",
    "        **performance_metrics[dataset_name]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88261129-37a2-4e0e-a8d1-8fc7ad490992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Dataset': 'reddit_50k.json', 'No. nodes': 4, 'Execution Time (s)': 0.27208399772644043, 'CPU Usage': 41.5, 'Memory Usage': 0.067291259765625, 'Memory Peak': 0.07555770874023438}, {'Dataset': 'reddit_100k.json', 'No. nodes': 4, 'Execution Time (s)': 0.2372734546661377, 'CPU Usage': 34.5, 'Memory Usage': 0.043979644775390625, 'Memory Peak': 0.07555770874023438}, {'Dataset': 'reddit_200k.json', 'No. nodes': 4, 'Execution Time (s)': 0.14737772941589355, 'CPU Usage': 27.8, 'Memory Usage': 0.00048065185546875, 'Memory Peak': 0.07563018798828125}]\n"
     ]
    }
   ],
   "source": [
    "print(metrics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3858ac-197d-496c-b8c1-262dcd4bc833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: df_400k\n",
      "\n",
      "Processing dataset: df_800k\n"
     ]
    }
   ],
   "source": [
    "## Do the 400k and 800k datasets here (SEPARATELY)\n",
    "\n",
    "#Code for importing / creating the datasets\n",
    "# Load full 500k dataset\n",
    "# df_500k = spark_session.read.json(\"hdfs://192.168.2.156:9000/data/reddit/reddit_500k.json\")\n",
    "# # Sample 4/5 (80%) of the dataset to get ~400K rows\n",
    "# df_400k = df_500k.sample(fraction=0.8, seed=42)\n",
    "\n",
    "# Load full dataset (19.8 GB)\n",
    "df_full = spark_session.read.json(\"hdfs://192.168.2.156:9000/data/reddit/corpus-webis-tldr-17.json\")\n",
    "\n",
    "# Sample approximately 400K rows\n",
    "df_400k = df_full.sample(fraction=(400000 / df_full.count()), seed=42)\n",
    "\n",
    "# Sample approximately 800K rows\n",
    "df_800k = df_full.sample(fraction=(800000 / df_full.count()), seed=42)\n",
    "\n",
    "#dataset_path2 = [df_400k, df_800k]\n",
    "\n",
    "\n",
    "# # Run the preprocess and the measurements\n",
    "\n",
    "# reddit_df = preprocess_data(400k)\n",
    "# performance_metrics = measure_performance(reddit_df, dataset_name, {})\n",
    "\n",
    "# # Save the results to the SAME list : metrics_result\n",
    "# metrics_result.append({\n",
    "#     \"Dataset\" : dataset_name, \n",
    "#     \"No. nodes\": 1,   # <-------------- CHANGE FOR DIFFERENT NO. NODES\n",
    "#     **performance_metrics[dataset_name]\n",
    "# })\n",
    "\n",
    "# # THEN save the file in the results folder by running the next block  |\n",
    "# #                                                                     V\n",
    "\n",
    "# List of dataset names corresponding to the 400k and 800k DataFrames\n",
    "dataset_names = [\"df_400k\", \"df_800k\"]\n",
    "\n",
    "# List of DataFrames to process\n",
    "dataset_path2 = [df_400k, df_800k]\n",
    "\n",
    "for i, dataset in enumerate(dataset_path2):\n",
    "    dataset_name = dataset_names[i]  # Use the predefined dataset names\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "\n",
    "    # Preprocess data\n",
    "    reddit_df2 = preprocess_data(dataset)  # Ensure you pass the DataFrame, not a column\n",
    "\n",
    "    # Measure performance & store results\n",
    "    performance_metrics2 = measure_performance(reddit_df2, dataset_name, {})\n",
    "\n",
    "    metrics_result.append({\n",
    "        \"Dataset\": dataset_name, \n",
    "        \"No. nodes\": 4,  # Change for different number of nodes\n",
    "        **performance_metrics2[dataset_name]\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fca04e2-d50b-4910-a1b5-f7bd5ab8948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Dataset': 'reddit_50k.json', 'No. nodes': 4, 'Execution Time (s)': 0.27208399772644043, 'CPU Usage': 41.5, 'Memory Usage': 0.067291259765625, 'Memory Peak': 0.07555770874023438}, {'Dataset': 'reddit_100k.json', 'No. nodes': 4, 'Execution Time (s)': 0.2372734546661377, 'CPU Usage': 34.5, 'Memory Usage': 0.043979644775390625, 'Memory Peak': 0.07555770874023438}, {'Dataset': 'reddit_200k.json', 'No. nodes': 4, 'Execution Time (s)': 0.14737772941589355, 'CPU Usage': 27.8, 'Memory Usage': 0.00048065185546875, 'Memory Peak': 0.07563018798828125}, {'Dataset': 'df_400k', 'No. nodes': 4, 'Execution Time (s)': 0.2130265235900879, 'CPU Usage': 21.400000000000002, 'Memory Usage': 0.0, 'Memory Peak': 0.075775146484375}, {'Dataset': 'df_800k', 'No. nodes': 4, 'Execution Time (s)': 0.1454622745513916, 'CPU Usage': -24.3, 'Memory Usage': 0.0, 'Memory Peak': 0.075775146484375}]\n"
     ]
    }
   ],
   "source": [
    "print(metrics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723ff0b4-22fd-4b8f-a535-a5465bedc275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance results saved to ./results/results_4node.csv\n"
     ]
    }
   ],
   "source": [
    "# Save result to CSV file\n",
    "csv_filename = \"./results/results_4node.csv\" # <--------- CHANGE FOR DIFFERENT NO. NODES\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Dataset\", \"No. nodes\", \"Execution Time (s)\", \"CPU Usage\", \"Memory Usage\", \"Memory Peak\", \"Total Cores Used\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(metrics_result)\n",
    "\n",
    "print(f\"\\nPerformance results saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b4cba33-542a-4f32-a79a-cabaa54fbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate session\n",
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a3f0c-6683-4665-956b-0b311e7aee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
